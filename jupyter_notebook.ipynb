{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc34e8b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T18:18:31.640265Z",
     "iopub.status.busy": "2025-10-13T18:18:31.639769Z",
     "iopub.status.idle": "2025-10-13T18:18:35.935421Z",
     "shell.execute_reply": "2025-10-13T18:18:35.934670Z"
    },
    "papermill": {
     "duration": 4.301929,
     "end_time": "2025-10-13T18:18:35.936799",
     "exception": false,
     "start_time": "2025-10-13T18:18:31.634870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn xgboost tqdm joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d628908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T18:18:35.942113Z",
     "iopub.status.busy": "2025-10-13T18:18:35.941535Z",
     "iopub.status.idle": "2025-10-13T18:18:42.947006Z",
     "shell.execute_reply": "2025-10-13T18:18:42.946356Z"
    },
    "papermill": {
     "duration": 7.00937,
     "end_time": "2025-10-13T18:18:42.948379",
     "exception": false,
     "start_time": "2025-10-13T18:18:35.939009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0) Imports\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9906a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T18:18:42.954162Z",
     "iopub.status.busy": "2025-10-13T18:18:42.953572Z",
     "iopub.status.idle": "2025-10-13T18:23:51.293735Z",
     "shell.execute_reply": "2025-10-13T18:23:51.293006Z"
    },
    "papermill": {
     "duration": 308.344555,
     "end_time": "2025-10-13T18:23:51.295401",
     "exception": false,
     "start_time": "2025-10-13T18:18:42.950846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Load CSV\n",
    "# -----------------------------\n",
    "CSV_PATH = \"/kaggle/input/train-16/train_embed_16.csv\"  # replace with your path\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Convert embeddings to numpy arrays and handle missing\n",
    "# -----------------------------\n",
    "def to_array_fill_zero(x, expected_dim=1024):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            arr = np.array(literal_eval(x), dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array(x, dtype=np.float32)\n",
    "    except Exception:\n",
    "        arr = np.zeros(expected_dim, dtype=np.float32)\n",
    "    if arr.size != expected_dim:\n",
    "        return np.zeros(expected_dim, dtype=np.float32)\n",
    "    return arr\n",
    "\n",
    "df['embed_array'] = df['concatenate_embedding'].apply(lambda x: to_array_fill_zero(x, 1024))\n",
    "X_base = np.vstack(df['embed_array'].values)\n",
    "\n",
    "# Add extra features: norm, mean, std\n",
    "extra_features = np.vstack([np.linalg.norm(X_base, axis=1),\n",
    "                            np.mean(X_base, axis=1),\n",
    "                            np.std(X_base, axis=1)]).T\n",
    "X = np.hstack([X_base, extra_features])  # 1027-dim\n",
    "y = np.log1p(df['price'].values)  # log-transform target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35c9432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T18:23:51.302153Z",
     "iopub.status.busy": "2025-10-13T18:23:51.301731Z",
     "iopub.status.idle": "2025-10-13T18:23:52.843833Z",
     "shell.execute_reply": "2025-10-13T18:23:52.843178Z"
    },
    "papermill": {
     "duration": 1.546698,
     "end_time": "2025-10-13T18:23:52.845184",
     "exception": false,
     "start_time": "2025-10-13T18:23:51.298486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) Scale features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) PyTorch Dataset\n",
    "# -----------------------------\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1,1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = PriceDataset(X_scaled, y)\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f855ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T18:23:52.850451Z",
     "iopub.status.busy": "2025-10-13T18:23:52.850246Z",
     "iopub.status.idle": "2025-10-13T19:33:43.483741Z",
     "shell.execute_reply": "2025-10-13T19:33:43.482786Z"
    },
    "papermill": {
     "duration": 4190.637793,
     "end_time": "2025-10-13T19:33:43.485146",
     "exception": false,
     "start_time": "2025-10-13T18:23:52.847353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ensemble...\n",
      "\n",
      "Training ensemble models...\n",
      "\n",
      "Training model 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.2954, val_loss=0.2278, val_smape=57.72\n",
      "Epoch 10: train_loss=0.1859, val_loss=0.2033, val_smape=51.49\n",
      "Epoch 20: train_loss=0.1539, val_loss=0.1973, val_smape=49.59\n",
      "Epoch 30: train_loss=0.1255, val_loss=0.1941, val_smape=48.83\n",
      "Epoch 40: train_loss=0.1130, val_loss=0.1942, val_smape=48.68\n",
      "Epoch 50: train_loss=0.1051, val_loss=0.1936, val_smape=48.30\n",
      "Epoch 60: train_loss=0.0996, val_loss=0.1931, val_smape=48.06\n",
      "Epoch 70: train_loss=0.0898, val_loss=0.1918, val_smape=47.76\n",
      "Epoch 80: train_loss=0.0861, val_loss=0.1929, val_smape=47.98\n",
      "Epoch 90: train_loss=0.0815, val_loss=0.1906, val_smape=47.61\n",
      "Epoch 100: train_loss=0.0797, val_loss=0.1920, val_smape=47.71\n",
      "Epoch 110: train_loss=0.0775, val_loss=0.1905, val_smape=47.54\n",
      "Epoch 120: train_loss=0.0766, val_loss=0.1916, val_smape=47.62\n",
      "Epoch 130: train_loss=0.0751, val_loss=0.1923, val_smape=47.69\n",
      "Epoch 140: train_loss=0.0750, val_loss=0.1922, val_smape=47.69\n",
      "Model 1 best validation SMAPE: 47.40\n",
      "\n",
      "Training model 2/5\n",
      "Epoch 0: train_loss=0.2916, val_loss=0.2275, val_smape=57.70\n",
      "Epoch 10: train_loss=0.1828, val_loss=0.2029, val_smape=51.26\n",
      "Epoch 20: train_loss=0.1502, val_loss=0.1994, val_smape=50.01\n",
      "Epoch 30: train_loss=0.1308, val_loss=0.1959, val_smape=48.95\n",
      "Epoch 40: train_loss=0.1181, val_loss=0.1969, val_smape=49.22\n",
      "Epoch 50: train_loss=0.1094, val_loss=0.1955, val_smape=48.70\n",
      "Epoch 60: train_loss=0.0945, val_loss=0.1939, val_smape=48.24\n",
      "Epoch 70: train_loss=0.0881, val_loss=0.1923, val_smape=47.88\n",
      "Epoch 80: train_loss=0.0844, val_loss=0.1949, val_smape=48.34\n",
      "Epoch 90: train_loss=0.0778, val_loss=0.1917, val_smape=47.89\n",
      "Epoch 100: train_loss=0.0759, val_loss=0.1916, val_smape=47.76\n",
      "Epoch 110: train_loss=0.0727, val_loss=0.1913, val_smape=47.57\n",
      "Epoch 120: train_loss=0.0712, val_loss=0.1913, val_smape=47.70\n",
      "Epoch 130: train_loss=0.0706, val_loss=0.1904, val_smape=47.44\n",
      "Epoch 140: train_loss=0.0688, val_loss=0.1908, val_smape=47.47\n",
      "Model 2 best validation SMAPE: 47.30\n",
      "\n",
      "Training model 3/5\n",
      "Epoch 0: train_loss=0.3452, val_loss=0.2288, val_smape=57.78\n",
      "Epoch 10: train_loss=0.1869, val_loss=0.2037, val_smape=51.43\n",
      "Epoch 20: train_loss=0.1548, val_loss=0.2008, val_smape=50.26\n",
      "Epoch 30: train_loss=0.1364, val_loss=0.1981, val_smape=49.43\n",
      "Epoch 40: train_loss=0.1165, val_loss=0.1949, val_smape=48.78\n",
      "Epoch 50: train_loss=0.1074, val_loss=0.1936, val_smape=48.31\n",
      "Epoch 60: train_loss=0.0996, val_loss=0.1931, val_smape=48.14\n",
      "Epoch 70: train_loss=0.0919, val_loss=0.1942, val_smape=48.45\n",
      "Epoch 80: train_loss=0.0887, val_loss=0.1949, val_smape=48.27\n",
      "Epoch 90: train_loss=0.0865, val_loss=0.1936, val_smape=48.31\n",
      "Epoch 100: train_loss=0.0848, val_loss=0.1917, val_smape=47.82\n",
      "Epoch 110: train_loss=0.0838, val_loss=0.1926, val_smape=48.03\n",
      "Epoch 120: train_loss=0.0837, val_loss=0.1922, val_smape=47.91\n",
      "Early stopping at epoch 125\n",
      "Model 3 best validation SMAPE: 47.82\n",
      "\n",
      "Training model 4/5\n",
      "Epoch 0: train_loss=0.3008, val_loss=0.2289, val_smape=58.27\n",
      "Epoch 10: train_loss=0.1872, val_loss=0.2048, val_smape=51.94\n",
      "Epoch 20: train_loss=0.1566, val_loss=0.1972, val_smape=49.63\n",
      "Epoch 30: train_loss=0.1376, val_loss=0.1955, val_smape=48.91\n",
      "Epoch 40: train_loss=0.1148, val_loss=0.1928, val_smape=48.21\n",
      "Epoch 50: train_loss=0.1064, val_loss=0.1920, val_smape=47.91\n",
      "Epoch 60: train_loss=0.1005, val_loss=0.1892, val_smape=47.24\n",
      "Epoch 70: train_loss=0.0932, val_loss=0.1902, val_smape=47.47\n",
      "Epoch 80: train_loss=0.0859, val_loss=0.1895, val_smape=47.18\n",
      "Epoch 90: train_loss=0.0833, val_loss=0.1899, val_smape=47.38\n",
      "Epoch 100: train_loss=0.0808, val_loss=0.1894, val_smape=47.12\n",
      "Epoch 110: train_loss=0.0800, val_loss=0.1892, val_smape=47.13\n",
      "Epoch 120: train_loss=0.0787, val_loss=0.1889, val_smape=47.19\n",
      "Epoch 130: train_loss=0.0779, val_loss=0.1906, val_smape=47.59\n",
      "Epoch 140: train_loss=0.0783, val_loss=0.1891, val_smape=47.05\n",
      "Model 4 best validation SMAPE: 46.96\n",
      "\n",
      "Training model 5/5\n",
      "Epoch 0: train_loss=0.3705, val_loss=0.2311, val_smape=58.61\n",
      "Epoch 10: train_loss=0.1867, val_loss=0.2032, val_smape=51.27\n",
      "Epoch 20: train_loss=0.1567, val_loss=0.1987, val_smape=49.99\n",
      "Epoch 30: train_loss=0.1381, val_loss=0.1976, val_smape=49.46\n",
      "Epoch 40: train_loss=0.1184, val_loss=0.1948, val_smape=48.59\n",
      "Epoch 50: train_loss=0.1059, val_loss=0.1921, val_smape=47.99\n",
      "Epoch 60: train_loss=0.0999, val_loss=0.1913, val_smape=47.89\n",
      "Epoch 70: train_loss=0.0958, val_loss=0.1920, val_smape=48.00\n",
      "Epoch 80: train_loss=0.0935, val_loss=0.1924, val_smape=47.89\n",
      "Epoch 90: train_loss=0.0931, val_loss=0.1927, val_smape=47.95\n",
      "Epoch 100: train_loss=0.0923, val_loss=0.1932, val_smape=48.10\n",
      "Epoch 110: train_loss=0.0926, val_loss=0.1928, val_smape=48.11\n",
      "Epoch 120: train_loss=0.0921, val_loss=0.1928, val_smape=48.12\n",
      "Epoch 130: train_loss=0.0923, val_loss=0.1920, val_smape=47.96\n",
      "Epoch 140: train_loss=0.0917, val_loss=0.1919, val_smape=47.95\n",
      "Early stopping at epoch 141\n",
      "Model 5 best validation SMAPE: 47.77\n",
      "\n",
      "=== Final Ensemble Evaluation ===\n",
      "Individual model SMAPEs: ['47.40', '47.30', '47.82', '46.96', '47.77']\n",
      "Ensemble Validation SMAPE: 46.23\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Improved SMAPE Loss\n",
    "# -----------------------------\n",
    "class SMAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        abs_diff = torch.abs(y_pred - y_true)\n",
    "        denominator = (torch.abs(y_pred) + torch.abs(y_true)) / 2 + self.eps\n",
    "        return torch.mean(abs_diff / denominator)\n",
    "\n",
    "criterion = SMAPELoss()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Enhanced MLP Model\n",
    "# -----------------------------\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[2048, 1024, 512, 256, 128], dropouts=[0.4, 0.4, 0.3, 0.2, 0.1]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, (hidden_dim, dropout) in enumerate(zip(hidden_dims, dropouts)):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Improved Training Function\n",
    "# -----------------------------\n",
    "def train_mlp_improved(model, train_loader, val_loader, n_epochs=30, lr=1e-3, patience=25):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8, verbose=True)\n",
    "    \n",
    "    best_val_smape = np.inf\n",
    "    stop_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_val_pred_list, y_val_true_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                val_loss = criterion(pred, yb)\n",
    "                val_losses.append(val_loss.item())\n",
    "                y_val_pred_list.append(pred.cpu().numpy())\n",
    "                y_val_true_list.append(yb.cpu().numpy())\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Calculate SMAPE on original scale\n",
    "        y_val_pred_np = np.vstack(y_val_pred_list)\n",
    "        y_val_true_np = np.vstack(y_val_true_list)\n",
    "        \n",
    "        # Convert back from log1p if used during preprocessing\n",
    "        y_val_pred_orig = np.expm1(y_val_pred_np)  # Remove if no log transformation\n",
    "        y_val_true_orig = np.expm1(y_val_true_np)  # Remove if no log transformation\n",
    "        \n",
    "        val_smape_epoch = 100 * np.mean(\n",
    "            2 * np.abs(y_val_pred_orig - y_val_true_orig) / \n",
    "            (np.abs(y_val_true_orig) + np.abs(y_val_pred_orig) + 1e-8)\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_smape_epoch < best_val_smape:\n",
    "            best_val_smape = val_smape_epoch\n",
    "            stop_counter = 0\n",
    "            torch.save(model.state_dict(), f\"/kaggle/working/best_mlp_{id(model)}.pt\")\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "            \n",
    "        if stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: train_loss={np.mean(train_losses):.4f}, val_loss={val_loss:.4f}, val_smape={val_smape_epoch:.2f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f\"/kaggle/working/best_mlp_{id(model)}.pt\"))\n",
    "    return model, best_val_smape\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Create Ensemble\n",
    "# -----------------------------\n",
    "def create_ensemble(input_dim):\n",
    "    models = []\n",
    "    # Different architectures\n",
    "    models.append(ImprovedMLP(input_dim, [2048, 1024, 512, 256, 128], [0.4, 0.4, 0.3, 0.2, 0.1]))\n",
    "    models.append(ImprovedMLP(input_dim, [1536, 768, 384, 192, 96], [0.35, 0.35, 0.25, 0.15, 0.05]))\n",
    "    models.append(ImprovedMLP(input_dim, [1024, 1024, 512, 256, 128, 64], [0.3, 0.3, 0.25, 0.2, 0.15, 0.1]))\n",
    "    models.append(ImprovedMLP(input_dim, [2560, 1280, 640, 320, 160], [0.45, 0.4, 0.35, 0.25, 0.15]))\n",
    "    models.append(ImprovedMLP(input_dim, [1024, 512, 256, 128, 64, 32], [0.3, 0.25, 0.2, 0.15, 0.1, 0.05]))\n",
    "    \n",
    "    return [model.to(device) for model in models]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Improved Ensemble Prediction\n",
    "# -----------------------------\n",
    "def ensemble_predict_improved(models, X_tensor):\n",
    "    \"\"\"Ensemble prediction with median + mean combination\"\"\"\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = m(X_tensor).cpu().numpy().flatten()\n",
    "            preds.append(pred)\n",
    "    \n",
    "    preds_array = np.array(preds)\n",
    "    \n",
    "    # Use median to reduce outlier effects, then average\n",
    "    median_preds = np.median(preds_array, axis=0)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "    \n",
    "    # Combine median and mean for robustness\n",
    "    final_preds = 0.7 * median_preds + 0.3 * mean_preds\n",
    "    return final_preds\n",
    "\n",
    "# -----------------------------\n",
    "# 6) MAIN TRAINING PIPELINE\n",
    "# -----------------------------\n",
    "print(\"Creating ensemble...\")\n",
    "ensemble_models = create_ensemble(X_scaled.shape[1])\n",
    "ensemble_val_smapes = []\n",
    "\n",
    "print(\"\\nTraining ensemble models...\")\n",
    "for i, model in enumerate(ensemble_models):\n",
    "    print(f\"\\nTraining model {i+1}/{len(ensemble_models)}\")\n",
    "    trained_model, best_smape = train_mlp_improved(\n",
    "        model, train_loader, val_loader, n_epochs=150, lr=1e-3, patience=25\n",
    "    )\n",
    "    ensemble_val_smapes.append(best_smape)\n",
    "    print(f\"Model {i+1} best validation SMAPE: {best_smape:.2f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) FINAL EVALUATION\n",
    "# -----------------------------\n",
    "print(\"\\n=== Final Ensemble Evaluation ===\")\n",
    "\n",
    "# Prepare validation data\n",
    "X_val_tensor = torch.tensor(np.vstack([x for x,_ in val_dataset]), dtype=torch.float32).to(device)\n",
    "y_val_true_log = np.vstack([y for _,y in val_dataset]).flatten()\n",
    "\n",
    "# Get ensemble predictions (in log scale)\n",
    "y_val_pred_log = ensemble_predict_improved(ensemble_models, X_val_tensor)\n",
    "\n",
    "# Convert back to original scale\n",
    "y_val_pred_orig = np.expm1(y_val_pred_log)\n",
    "y_val_true_orig = np.expm1(y_val_true_log)\n",
    "\n",
    "# Final SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "val_smape = smape(y_val_true_orig, y_val_pred_orig)\n",
    "print(f\"Individual model SMAPEs: {[f'{s:.2f}' for s in ensemble_val_smapes]}\")\n",
    "print(f\"Ensemble Validation SMAPE: {val_smape:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc9e9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T19:33:43.496421Z",
     "iopub.status.busy": "2025-10-13T19:33:43.496084Z",
     "iopub.status.idle": "2025-10-13T19:38:56.827923Z",
     "shell.execute_reply": "2025-10-13T19:38:56.827133Z"
    },
    "papermill": {
     "duration": 313.342724,
     "end_time": "2025-10-13T19:38:56.833068",
     "exception": false,
     "start_time": "2025-10-13T19:33:43.490344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predicted prices -> /kaggle/working/final_test_out.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 0) Helper to convert embedding and fill empty with zeros\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def to_array_fill_zero(x, size=1024):\n",
    "    if isinstance(x, str) and len(x) > 2:\n",
    "        return np.array(literal_eval(x), dtype=np.float32)\n",
    "    else:\n",
    "        return np.zeros(size, dtype=np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load new CSV\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "NEW_CSV = \"/kaggle/input/test-16/test_embed_16.csv\"  # replace with your path\n",
    "df_test = pd.read_csv(NEW_CSV)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Convert embeddings\n",
    "# -----------------------------\n",
    "df_test['embed_array'] = df_test['concatenate_embedding'].apply(lambda x: to_array_fill_zero(x, 1024))\n",
    "X_base_test = np.vstack(df_test['embed_array'].values)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Extra features (optional, same as training)\n",
    "# -----------------------------\n",
    "extra_features_test = np.vstack([\n",
    "    np.linalg.norm(X_base_test, axis=1),\n",
    "    np.mean(X_base_test, axis=1),\n",
    "    np.std(X_base_test, axis=1)\n",
    "]).T\n",
    "\n",
    "X_test = np.hstack([X_base_test, extra_features_test])\n",
    "X_test_scaled = scaler.transform(X_test)  # use the scaler from training\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Ensemble prediction\n",
    "# -----------------------------\n",
    "import torch\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "def ensemble_predict(models, X_tensor):\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            preds.append(m(X_tensor).cpu().numpy().flatten())\n",
    "    return np.mean(np.vstack(preds), axis=0)\n",
    "\n",
    "pred_log = ensemble_predict(ensemble_models, X_test_tensor)  # ensemble of 5 models\n",
    "pred_price = np.expm1(pred_log)  # inverse of log1p if used during training\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save CSV\n",
    "# -----------------------------\n",
    "df_test['price'] = pred_price\n",
    "df_test[['sample_id', 'price']].to_csv(\"/kaggle/working/final_test_out.csv\", index=False)\n",
    "print(\"Saved predicted prices -> /kaggle/working/final_test_out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8481473,
     "sourceId": 13369494,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8484838,
     "sourceId": 13373785,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4831.672343,
   "end_time": "2025-10-13T19:38:59.538285",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-13T18:18:27.865942",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
